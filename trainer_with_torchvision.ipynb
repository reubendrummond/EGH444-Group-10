{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for deep learning and visualization\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Scientific computing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# HuggingFace Transformers for SegFormer\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "\n",
    "DATA_ROOT = \"./datasets/NYUDepthv2\"\n",
    "\n",
    "if os.getcwd() == \"/kaggle/working\":\n",
    "    DATA_ROOT = \"/kaggle/input/nyudepthv2/NYUDepthv2\"\n",
    "\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "if COLAB:\n",
    "    assert torch.cuda.is_available(), \"Colab session must have a GPU enabled.\"\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    DATA_ROOT = \"drive/MyDrive/EGH444/datasets/NYUDepthv2\"\n",
    "    \n",
    "assert os.path.exists(DATA_ROOT), f\"Data root {DATA_ROOT} does not exist.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Shared Configuration System\n",
    "\n",
    "Let's create a comprehensive configuration system that allows us to easily switch between different experimental setups. This approach ensures reproducibility and makes it easy to compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration classes defined!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainingHistory:\n",
    "    \"\"\"Tracks comprehensive training metrics for analysis and visualization.\"\"\"\n",
    "    \n",
    "    # Training metrics (per epoch)\n",
    "    train_loss: List[float] = field(default_factory=list)\n",
    "    train_miou: List[float] = field(default_factory=list)\n",
    "    train_pixacc: List[float] = field(default_factory=list)\n",
    "    \n",
    "    # Validation metrics (per epoch)\n",
    "    val_loss: List[float] = field(default_factory=list)\n",
    "    val_miou: List[float] = field(default_factory=list)\n",
    "    val_pixacc: List[float] = field(default_factory=list)\n",
    "    val_iou_per_class: List[List[float]] = field(default_factory=list)\n",
    "    \n",
    "    # Training metadata\n",
    "    learning_rates: List[float] = field(default_factory=list)\n",
    "    epoch_times: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def add_epoch(self, epoch_data: Dict[str, float]) -> None:\n",
    "        \"\"\"Add metrics for a completed epoch.\"\"\"\n",
    "        self.train_loss.append(epoch_data.get(\"train_loss\", 0.0))\n",
    "        self.train_miou.append(epoch_data.get(\"train_miou\", 0.0))\n",
    "        self.train_pixacc.append(epoch_data.get(\"train_pixacc\", 0.0))\n",
    "        \n",
    "        self.val_loss.append(epoch_data.get(\"val_loss\", 0.0))\n",
    "        self.val_miou.append(epoch_data.get(\"val_miou\", 0.0))\n",
    "        self.val_pixacc.append(epoch_data.get(\"val_pixacc\", 0.0))\n",
    "        self.val_iou_per_class.append(epoch_data.get(\"val_iou_per_class\", []))\n",
    "        \n",
    "        self.learning_rates.append(epoch_data.get(\"learning_rate\", 0.0))\n",
    "        self.epoch_times.append(epoch_data.get(\"epoch_time\", 0.0))\n",
    "    \n",
    "    def get_best_epoch(self, metric: str = \"val_miou\") -> Tuple[int, float]:\n",
    "        \"\"\"Get the epoch number and value of the best performance.\"\"\"\n",
    "        values = getattr(self, metric, [])\n",
    "        if not values:\n",
    "            return 0, 0.0\n",
    "        best_idx = max(range(len(values)), key=lambda i: values[i])\n",
    "        return best_idx + 1, values[best_idx]  # 1-indexed epoch\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of completed epochs.\"\"\"\n",
    "        return len(self.train_loss)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Self-contained training configuration that includes data setup.\"\"\"\n",
    "    \n",
    "    model_name: str  # Simple model identifier for checkpoint organization\n",
    "    epochs: int\n",
    "    batch_size: int = 32\n",
    "    \n",
    "    processor: Optional[Any] = None  # SegFormer processor or similar (optional for custom datasets)\n",
    "    data_root: str = DATA_ROOT\n",
    "    image_size: Tuple[int, int] = (240, 320)  # (height, width)\n",
    "    num_workers: int = 0\n",
    "    pin_memory: bool = True\n",
    "    \n",
    "    learning_rate: float = 1e-4\n",
    "    \n",
    "    num_classes: int = 40\n",
    "    ignore_index: int = 255\n",
    "    device: str = \"auto\"\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Logging/checkpointing\n",
    "    log_every: int = 5\n",
    "    save_dir: str = \"checkpoints\"\n",
    "    \n",
    "    @property\n",
    "    def model_dir(self) -> Path:\n",
    "        \"\"\"Get model-specific directory.\"\"\"\n",
    "        return Path(self.save_dir) / self.model_name\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create save directory after initialization.\"\"\"\n",
    "        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "print(\"‚úÖ Configuration classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Device Setup & Seed Management\n",
    "\n",
    "Proper device detection and seed management are crucial for reproducible experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÇ Section 2: Data Loading & Transformations\n",
    "\n",
    "NYUv2 dataset loading with SegFormer-compatible preprocessing and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(batch: Dict[str, torch.Tensor],\n",
    "                    num_samples: int = 4) -> None:\n",
    "    \"\"\"Visualize samples from a batch - supports both SegFormer and TorchVision formats.\"\"\"\n",
    "    \n",
    "    # ImageNet normalization constants\n",
    "    IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    def denormalize_imagenet(tensor: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Denormalize ImageNet preprocessed image.\"\"\"\n",
    "        img = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img * IMAGENET_STD + IMAGENET_MEAN).clip(0, 1)\n",
    "        return (img * 255).astype(np.uint8)\n",
    "    \n",
    "    def colorize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create colored visualization of segmentation mask.\"\"\"\n",
    "        np.random.seed(42)  # Consistent colors\n",
    "        colors = np.random.randint(0, 255, (41, 3), dtype=np.uint8)  # 40 classes + ignore\n",
    "        colors[40] = [0, 0, 0]  # Ignore class = black\n",
    "        \n",
    "        colored = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for class_id in range(41):\n",
    "            colored[mask == class_id] = colors[class_id]\n",
    "        colored[mask == 255] = [128, 128, 128]  # Ignore = gray\n",
    "        \n",
    "        return colored\n",
    "    \n",
    "    # Detect batch format and extract data\n",
    "    if \"pixel_values\" in batch:\n",
    "        # SegFormer format\n",
    "        images = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"] \n",
    "        ids = batch[\"id\"]\n",
    "        format_name = \"SegFormer\"\n",
    "    elif \"image\" in batch:\n",
    "        # TorchVision format\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"mask\"]\n",
    "        ids = batch[\"id\"]\n",
    "        format_name = \"TorchVision\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown batch format. Expected 'pixel_values' or 'image' keys.\")\n",
    "    \n",
    "    num_samples = min(num_samples, images.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 4, 8))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Denormalize image (both formats use ImageNet normalization)\n",
    "        img = denormalize_imagenet(images[i])\n",
    "        mask = labels[i].cpu().numpy()\n",
    "        colored_mask = colorize_mask(mask)\n",
    "        \n",
    "        # Plot image\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title(f\"Image: {ids[i]}\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Plot mask\n",
    "        axes[1, i].imshow(colored_mask)\n",
    "        axes[1, i].set_title(f\"Mask: {mask.shape}\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(f\"Batch Visualization ({format_name} format)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nyu40_ids(mask_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure mask uses NYUv2-40 label space [0..39]. Out-of-range ‚Üí 255 (ignore).\"\"\"\n",
    "    out = mask_np.astype(np.int64, copy=True)\n",
    "    bad = (out < 0) | (out > 39)\n",
    "    out[bad] = 255\n",
    "    return out\n",
    "\n",
    "\n",
    "class SegFormerTransforms:\n",
    "    \"\"\"Enhanced data augmentation transforms for SegFormer training.\"\"\"\n",
    "    \n",
    "    class Train:\n",
    "        \"\"\"Training augmentations with RGB and depth support.\"\"\"\n",
    "        def __init__(self, hflip_p=0.5, rotation_p=0.3, rotation_angle=15.0, \n",
    "                     color_jitter_p=0.4, brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1,\n",
    "                     crop_p=0.3, crop_scale=(0.8, 1.0), \n",
    "                     use_depth=False, depth_noise_p=0.3, depth_noise_std=0.02, depth_dropout_p=0.2):\n",
    "            self.hflip_p = hflip_p\n",
    "            self.rotation_p = rotation_p\n",
    "            self.rotation_angle = rotation_angle\n",
    "            self.color_jitter_p = color_jitter_p\n",
    "            self.brightness = brightness\n",
    "            self.contrast = contrast\n",
    "            self.saturation = saturation\n",
    "            self.hue = hue\n",
    "            self.crop_p = crop_p\n",
    "            self.crop_scale = crop_scale\n",
    "            self.use_depth = use_depth\n",
    "            self.depth_noise_p = depth_noise_p\n",
    "            self.depth_noise_std = depth_noise_std\n",
    "            self.depth_dropout_p = depth_dropout_p\n",
    "        \n",
    "        def _apply_crop(self, rgb: np.ndarray, label: np.ndarray, depth: np.ndarray = None):\n",
    "            \"\"\"Apply random crop with zoom.\"\"\"\n",
    "            h, w = rgb.shape[:2]\n",
    "            scale = random.uniform(*self.crop_scale)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            \n",
    "            # Random crop position\n",
    "            top = random.randint(0, max(0, h - new_h))\n",
    "            left = random.randint(0, max(0, w - new_w))\n",
    "            \n",
    "            # Crop and resize back\n",
    "            rgb_crop = rgb[top:top+new_h, left:left+new_w]\n",
    "            label_crop = label[top:top+new_h, left:left+new_w]\n",
    "            rgb = cv2.resize(rgb_crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            label = cv2.resize(label_crop.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST).astype(np.int64)\n",
    "            \n",
    "            if depth is not None:\n",
    "                depth_crop = depth[top:top+new_h, left:left+new_w]\n",
    "                depth = cv2.resize(depth_crop, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "                return rgb, label, depth\n",
    "            \n",
    "            return rgb, label\n",
    "        \n",
    "        def _apply_color_jitter(self, rgb: np.ndarray) -> np.ndarray:\n",
    "            \"\"\"Apply color jittering augmentations.\"\"\"\n",
    "            rgb = rgb.astype(np.float32)\n",
    "            \n",
    "            # Brightness\n",
    "            if self.brightness > 0:\n",
    "                brightness_factor = random.uniform(1 - self.brightness, 1 + self.brightness)\n",
    "                rgb *= brightness_factor\n",
    "            \n",
    "            # Contrast\n",
    "            if self.contrast > 0:\n",
    "                contrast_factor = random.uniform(1 - self.contrast, 1 + self.contrast)\n",
    "                mean = rgb.mean()\n",
    "                rgb = (rgb - mean) * contrast_factor + mean\n",
    "            \n",
    "            # Saturation (convert to HSV)\n",
    "            if self.saturation > 0:\n",
    "                hsv = cv2.cvtColor(np.clip(rgb, 0, 255).astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "                saturation_factor = random.uniform(1 - self.saturation, 1 + self.saturation)\n",
    "                hsv[:, :, 1] *= saturation_factor\n",
    "                hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)\n",
    "                rgb = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)\n",
    "            \n",
    "            # Hue shift\n",
    "            if self.hue > 0:\n",
    "                hsv = cv2.cvtColor(np.clip(rgb, 0, 255).astype(np.uint8), cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "                hue_shift = random.uniform(-self.hue, self.hue) * 179  # OpenCV hue is 0-179\n",
    "                hsv[:, :, 0] = (hsv[:, :, 0] + hue_shift) % 180\n",
    "                rgb = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB).astype(np.float32)\n",
    "            \n",
    "            return np.clip(rgb, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        def _apply_depth_augmentation(self, depth: np.ndarray) -> np.ndarray:\n",
    "            \"\"\"Apply depth-specific augmentations.\"\"\"\n",
    "            depth = depth.copy().astype(np.float32)\n",
    "            valid_mask = depth > 0\n",
    "            \n",
    "            # Gaussian noise\n",
    "            if random.random() < self.depth_noise_p and valid_mask.any():\n",
    "                noise = np.random.normal(0, self.depth_noise_std * 255, depth.shape).astype(np.float32)\n",
    "                depth[valid_mask] += noise[valid_mask]\n",
    "                depth = np.clip(depth, 0, 255)\n",
    "            \n",
    "            # Depth dropout (simulate sensor holes)\n",
    "            if random.random() < self.depth_dropout_p and valid_mask.any():\n",
    "                dropout_mask = np.random.random(depth.shape) < 0.05  # 5% dropout\n",
    "                depth[dropout_mask & valid_mask] = 0\n",
    "            \n",
    "            return depth\n",
    "        \n",
    "        def __call__(self, rgb: np.ndarray, label: np.ndarray, depth: np.ndarray = None) -> tuple:\n",
    "            h, w = rgb.shape[:2]\n",
    "            \n",
    "            # Random crop with zoom\n",
    "            if random.random() < self.crop_p:\n",
    "                if depth is not None:\n",
    "                    rgb, label, depth = self._apply_crop(rgb, label, depth)\n",
    "                else:\n",
    "                    rgb, label = self._apply_crop(rgb, label)\n",
    "            \n",
    "            # Horizontal flip\n",
    "            if random.random() < self.hflip_p:\n",
    "                rgb = np.ascontiguousarray(np.flip(rgb, axis=1))\n",
    "                label = np.ascontiguousarray(np.flip(label, axis=1))\n",
    "                if depth is not None:\n",
    "                    depth = np.ascontiguousarray(np.flip(depth, axis=1))\n",
    "            \n",
    "            # Rotation\n",
    "            if random.random() < self.rotation_p:\n",
    "                angle = random.uniform(-self.rotation_angle, self.rotation_angle)\n",
    "                center = (w // 2, h // 2)\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                rgb = cv2.warpAffine(rgb, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "                label = cv2.warpAffine(label.astype(np.uint8), M, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "                label = label.astype(np.int64)\n",
    "                if depth is not None:\n",
    "                    depth = cv2.warpAffine(depth, M, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "            \n",
    "            # Color jitter\n",
    "            if random.random() < self.color_jitter_p:\n",
    "                rgb = self._apply_color_jitter(rgb)\n",
    "            \n",
    "            # Depth augmentation\n",
    "            if depth is not None and self.use_depth:\n",
    "                depth = self._apply_depth_augmentation(depth)\n",
    "                return rgb, label, depth\n",
    "            \n",
    "            return rgb, label\n",
    "    \n",
    "    class Eval:\n",
    "        \"\"\"No augmentations for evaluation.\"\"\"\n",
    "        def __call__(self, rgb: np.ndarray, label: np.ndarray, depth: np.ndarray = None) -> tuple:\n",
    "            if depth is not None:\n",
    "                return rgb, label, depth\n",
    "            return rgb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms._presets import SemanticSegmentation\n",
    "\n",
    "class NYUDepthDataset(Dataset):\n",
    "    \"\"\"NYUv2 dataset with support for both SegFormer and TorchVision preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self, base_dir: str, split: str, processor: Any,\n",
    "                 transform=None, image_size: Optional[Tuple[int, int]] = None):\n",
    "        self.base = Path(base_dir)\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Detect processor type\n",
    "        self.is_segformer = isinstance(processor, SegformerImageProcessor) if processor else False\n",
    "        self.is_torchvision = isinstance(processor, SemanticSegmentation) if processor else False\n",
    "        \n",
    "        assert self.is_segformer or self.is_torchvision, \"Processor must be either SegFormer or TorchVision type.\"\n",
    "\n",
    "        # ensure split is valid\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"split must be 'train', 'val', or 'test'\"\n",
    "        folder_split = \"train\" if split == \"train\" or split == \"val\" else \"test\"\n",
    "\n",
    "        # Load split files\n",
    "        with open(self.base / f\"{folder_split}.txt\") as f:\n",
    "            stems = [Path(line.split()[0]).stem for line in f if line.strip()]\n",
    "\n",
    "        # If val split, take 20% of train set\n",
    "        if split == \"val\" or split == \"train\":\n",
    "            # Create reproducible random split\n",
    "            np.random.seed(42)  # Fixed seed for reproducibility\n",
    "            indices = np.random.permutation(len(stems))\n",
    "            val_size = int(0.2 * len(stems))\n",
    "\n",
    "            if split == \"val\":\n",
    "                stems = [stems[i] for i in indices[-val_size:]]  # Random 20%\n",
    "            else:  # train\n",
    "                stems = [stems[i] for i in indices[:-val_size]]  # Random 80%\n",
    "\n",
    "        # Find valid RGB + Label pairs\n",
    "        self.items = []\n",
    "        for s in stems:\n",
    "            rgb_path = self.base / \"RGB\" / f\"{s}.jpg\"\n",
    "            if not rgb_path.exists():\n",
    "                rgb_path = self.base / \"RGB\" / f\"{s}.png\"\n",
    "            label_path = self.base / \"Label\" / f\"{s}.png\"\n",
    "\n",
    "            if rgb_path.exists() and label_path.exists():\n",
    "                self.items.append((s, rgb_path, label_path))\n",
    "\n",
    "        if not self.items:\n",
    "            raise RuntimeError(f\"No valid samples found in {base_dir}/{folder_split}.txt\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        stem, rgb_path, label_path = self.items[idx]\n",
    "\n",
    "        # Load RGB and convert BGR‚ÜíRGB\n",
    "        rgb = cv2.imread(str(rgb_path), cv2.IMREAD_COLOR)\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load label and apply NYU40 processing\n",
    "        label = cv2.imread(str(label_path), cv2.IMREAD_GRAYSCALE).astype(np.int64)\n",
    "        label = to_nyu40_ids(label)\n",
    "\n",
    "        # Apply transforms if provided (before final processing)\n",
    "        if self.transform:\n",
    "            rgb, label = self.transform(rgb, label)\n",
    "\n",
    "        # Resize if custom size specified\n",
    "        if self.image_size:\n",
    "            h, w = self.image_size\n",
    "            rgb = cv2.resize(rgb, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            label = cv2.resize(label.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST).astype(np.int64)\n",
    "\n",
    "        # Handle different processor types\n",
    "        if self.is_segformer:\n",
    "            # SegFormer preprocessing\n",
    "            inputs = self.processor(images=rgb, return_tensors=\"pt\")\n",
    "            pixel_values = inputs[\"pixel_values\"].squeeze(0).contiguous()\n",
    "\n",
    "            # Process labels to match processor output size\n",
    "            processor_size = self.processor.size\n",
    "            if isinstance(processor_size, dict):\n",
    "                target_h, target_w = processor_size[\"height\"], processor_size[\"width\"]\n",
    "            else:\n",
    "                target_h = target_w = processor_size\n",
    "\n",
    "            label_resized = cv2.resize(\n",
    "                label.astype(np.uint8), (target_w, target_h), interpolation=cv2.INTER_NEAREST\n",
    "            )\n",
    "            labels = torch.from_numpy(label_resized.astype(np.int64)).contiguous()\n",
    "\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"labels\": labels,\n",
    "                \"id\": stem\n",
    "            }\n",
    "\n",
    "        elif self.is_torchvision:\n",
    "            # TorchVision preprocessing\n",
    "            from PIL import Image\n",
    "            rgb_pil = Image.fromarray(rgb)\n",
    "            label_pil = Image.fromarray(label.astype(np.uint8))\n",
    "\n",
    "            # Apply TorchVision transforms\n",
    "            image_tensor = self.processor(rgb_pil)\n",
    "\n",
    "            # For labels, we need to convert to tensor manually since TorchVision transforms are for images\n",
    "            labels = torch.from_numpy(label.astype(np.int64)).contiguous()\n",
    "\n",
    "            return {\n",
    "                \"image\": image_tensor,\n",
    "                \"mask\": labels,\n",
    "                \"id\": stem\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Processor must be provided and be either SegFormer or TorchVision type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_augmentations(dataset: NYUDepthDataset, sample_idx: int = 0, num_augmentations: int = 6, use_depth: bool = False):\n",
    "    \"\"\"Show multiple augmentations of a single sample without SegFormer preprocessing.\"\"\"\n",
    "    \n",
    "    # Get raw sample without transformss\n",
    "    stem, rgb_path, label_path = dataset.items[sample_idx]\n",
    "    \n",
    "    # Load RGB and label\n",
    "    rgb = cv2.imread(str(rgb_path), cv2.IMREAD_COLOR)\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    label = cv2.imread(str(label_path), cv2.IMREAD_GRAYSCALE).astype(np.int64)\n",
    "    label = to_nyu40_ids(label)\n",
    "    \n",
    "    # Load depth if requested\n",
    "    depth = None\n",
    "    if use_depth:\n",
    "        depth_path = dataset.base / \"Depth\" / f\"{stem}.png\"\n",
    "        if depth_path.exists():\n",
    "            depth = cv2.imread(str(depth_path), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Depth file not found: {depth_path}\")\n",
    "            use_depth = False\n",
    "    \n",
    "    # Create transform with enhanced settings for visualization\n",
    "    transform = SegFormerTransforms.Train(\n",
    "        hflip_p=0.8, rotation_p=0.8, rotation_angle=20,\n",
    "        color_jitter_p=0.8, brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15,\n",
    "        crop_p=0.6, crop_scale=(0.7, 1.0),\n",
    "        use_depth=use_depth, depth_noise_p=0.8, depth_noise_std=0.03, depth_dropout_p=0.5\n",
    "    )\n",
    "    \n",
    "    def colorize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Create colored visualization of segmentation mask.\"\"\"\n",
    "        np.random.seed(42)  # Consistent colors\n",
    "        colors = np.random.randint(0, 255, (41, 3), dtype=np.uint8)\n",
    "        colors[40] = [0, 0, 0]  # Ignore class = black\n",
    "        \n",
    "        colored = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for class_id in range(41):\n",
    "            colored[mask == class_id] = colors[class_id]\n",
    "        colored[mask == 255] = [128, 128, 128]  # Ignore = gray\n",
    "        return colored\n",
    "    \n",
    "    # Create visualization\n",
    "    num_cols = num_augmentations + 1  # +1 for original\n",
    "    num_rows = 3 if use_depth else 2  # RGB, Mask, (Depth)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 3))\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Show original\n",
    "    axes[0, 0].imshow(rgb)\n",
    "    axes[0, 0].set_title(\"Original RGB\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    \n",
    "    axes[1, 0].imshow(colorize_mask(label))\n",
    "    axes[1, 0].set_title(\"Original Mask\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "    \n",
    "    if use_depth and depth is not None:\n",
    "        axes[2, 0].imshow(depth, cmap='plasma')\n",
    "        axes[2, 0].set_title(\"Original Depth\")\n",
    "        axes[2, 0].axis(\"off\")\n",
    "    \n",
    "    # Show augmentations\n",
    "    for i in range(num_augmentations):\n",
    "        # Apply transform\n",
    "        if use_depth and depth is not None:\n",
    "            rgb_aug, label_aug, depth_aug = transform(rgb.copy(), label.copy(), depth.copy())\n",
    "        else:\n",
    "            rgb_aug, label_aug = transform(rgb.copy(), label.copy())\n",
    "        \n",
    "        # Display RGB\n",
    "        axes[0, i+1].imshow(rgb_aug)\n",
    "        axes[0, i+1].set_title(f\"Aug {i+1} RGB\")\n",
    "        axes[0, i+1].axis(\"off\")\n",
    "        \n",
    "        # Display mask\n",
    "        axes[1, i+1].imshow(colorize_mask(label_aug))\n",
    "        axes[1, i+1].set_title(f\"Aug {i+1} Mask\")\n",
    "        axes[1, i+1].axis(\"off\")\n",
    "        \n",
    "        # Display depth if available\n",
    "        if use_depth and depth is not None:\n",
    "            axes[2, i+1].imshow(depth_aug, cmap='plasma')\n",
    "            axes[2, i+1].set_title(f\"Aug {i+1} Depth\")\n",
    "            axes[2, i+1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Augmentations for sample: {stem}\", y=1.02, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Universal data loader creation function defined\n"
     ]
    }
   ],
   "source": [
    "def create_data_loaders(\n",
    "    data_root: str,\n",
    "    batch_size: int,\n",
    "    image_size: Tuple[int, int],\n",
    "    num_workers: int = 0,\n",
    "    processor = None,  # Can be SegFormer processor or TorchVision transforms\n",
    "    pin_memory: bool = True\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Create training, validation, and test data loaders.\n",
    "    \n",
    "    Args:\n",
    "        data_root: Path to dataset directory\n",
    "        processor: SegFormer processor or TorchVision transforms\n",
    "        batch_size: Batch size for data loaders\n",
    "        image_size: (height, width) for resizing\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        pin_memory: Whether to pin memory for faster GPU transfer\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detect processor type and print appropriate info\n",
    "    if processor is not None:\n",
    "        if hasattr(processor, 'size'):\n",
    "            print(f\"üìè Using SegFormer processor with size: {processor.size}\")\n",
    "        elif hasattr(processor, 'transforms'):\n",
    "            print(f\"üìè Using TorchVision transforms\")\n",
    "        else:\n",
    "            print(f\"üìè Using custom processor: {type(processor).__name__}\")\n",
    "    else:\n",
    "        print(\"üìè No processor provided - using default ImageNet normalization\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = SegFormerTransforms.Train()\n",
    "    val_transform = SegFormerTransforms.Eval()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NYUDepthDataset(\n",
    "        data_root, \"train\", processor, \n",
    "        transform=train_transform, image_size=image_size\n",
    "    )\n",
    "    val_dataset = NYUDepthDataset(\n",
    "        data_root, \"val\", processor,\n",
    "        transform=val_transform, image_size=image_size\n",
    "    )\n",
    "    test_dataset = NYUDepthDataset(\n",
    "        data_root, \"test\", processor,\n",
    "        transform=val_transform, image_size=image_size\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Train: {len(train_dataset)} samples,  Val: {len(val_dataset)} samples,  Test: {len(test_dataset)} samples\")\n",
    "    print(f\"üì¶ Train batches: {len(train_loader)},  Val batches: {len(val_loader)},  Test batches: {len(test_loader)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "print(\"‚úÖ Universal data loader creation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reubendrummond/Desktop/Uni/2025/Sem2/EGH444/EGH444-Group-10/venv/lib/python3.10/site-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NYUDepthDataset(\n",
    "    DATA_ROOT, \"train\", \n",
    "    processor=SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    ")\n",
    "# show_augmentations(train_dataset, sample_idx=10, num_augmentations=4, use_depth=False)\n",
    "# show_augmentations(train_dataset, sample_idx=10, num_augmentations=4, use_depth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Section 3: Universal Trainer Class\n",
    "\n",
    "A single, model-agnostic trainer that works with any PyTorch model and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Using SegFormer processor with size: {'height': 512, 'width': 512}\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 80,  Val batches: 20,  Test batches: 82\n",
      "\\nüß™ Testing data loading...\n",
      "‚úÖ Batch loaded successfully!\n",
      "üìè pixel_values shape: torch.Size([8, 3, 512, 512])\n",
      "üìè labels shape: torch.Size([8, 512, 512])\n",
      "üè∑Ô∏è Sample IDs: ['391', '1016', '256']\n",
      "\\nüñºÔ∏è Visualizing batch samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reubendrummond/Desktop/Uni/2025/Sem2/EGH444/EGH444-Group-10/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Example: Data exploration and visualization using standalone function\n",
    "model_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Create data loaders for exploration\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    data_root=DATA_ROOT,\n",
    "    processor=processor,\n",
    "    batch_size=8,\n",
    "    image_size=(240, 320),\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Test data loading and visualization\n",
    "print(\"\\\\nüß™ Testing data loading...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"‚úÖ Batch loaded successfully!\")\n",
    "print(f\"üìè pixel_values shape: {sample_batch['pixel_values'].shape}\")\n",
    "print(f\"üìè labels shape: {sample_batch['labels'].shape}\")\n",
    "print(f\"üè∑Ô∏è Sample IDs: {sample_batch['id'][:3]}\")\n",
    "\n",
    "# Visualize samples for Segformer\n",
    "print(\"\\\\nüñºÔ∏è Visualizing batch samples...\")\n",
    "# visualize_batch(sample_batch, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Universal Trainer class with dual model support defined!\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Universal trainer class with automatic checkpoint resume.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize trainer with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Training configuration with data and model setup\n",
    "            verbose: Whether to print detailed logs \n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.verbose = verbose\n",
    "        self.device = self._setup_device()\n",
    "        self.history = TrainingHistory()\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        self._set_random_seeds()\n",
    "        \n",
    "        # Create data loaders from config\n",
    "        self.train_loader, self.val_loader, self.test_loader = self._create_data_loaders()\n",
    "        \n",
    "        # Auto-resume setup\n",
    "        self.latest_checkpoint_path = self.config.model_dir / \"latest.pth\"\n",
    "        self.start_epoch = 1\n",
    "        self.loaded_checkpoint = None\n",
    "        \n",
    "        # Check for existing checkpoint\n",
    "        if self.latest_checkpoint_path.exists():\n",
    "            self._load_latest_checkpoint()\n",
    "    \n",
    "    def _setup_device(self) -> torch.device:\n",
    "        \"\"\"Setup training device.\"\"\"\n",
    "        if self.config.device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "            elif torch.backends.mps.is_available():\n",
    "                device = torch.device(\"cpu\")  # Temporary fix for MPS issues\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            device = torch.device(self.config.device)\n",
    "        print(f\"Using device: {device}\")\n",
    "        return device\n",
    "    \n",
    "    def _set_random_seeds(self) -> None:\n",
    "        \"\"\"Set random seeds for reproducible results.\"\"\"\n",
    "        random.seed(self.config.seed)\n",
    "        np.random.seed(self.config.seed)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        torch.cuda.manual_seed_all(self.config.seed)\n",
    "        \n",
    "        # Make CuDNN deterministic (slower but reproducible)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        print(f\"üé≤ Random seeds set to {self.config.seed} for reproducibility\")\n",
    "    \n",
    "    def _create_data_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"Create training, validation, and test data loaders from config.\"\"\"\n",
    "        \n",
    "        # Handle processor-based datasets (like SegFormer)\n",
    "        if self.config.processor is not None:\n",
    "            # Use the standalone create_data_loaders function\n",
    "            return create_data_loaders(\n",
    "                data_root=self.config.data_root,\n",
    "                processor=self.config.processor,\n",
    "                batch_size=self.config.batch_size,\n",
    "                image_size=self.config.image_size,\n",
    "                num_workers=self.config.num_workers,\n",
    "                pin_memory=self.config.pin_memory\n",
    "            )\n",
    "        else:\n",
    "            # For custom datasets without processor, you would implement your own dataset creation here\n",
    "            # This is a placeholder for when processor=None\n",
    "            raise NotImplementedError(\n",
    "                \"Custom dataset creation (processor=None) not implemented yet. \"\n",
    "                \"Please provide a processor or implement custom dataset logic.\"\n",
    "            )\n",
    "    \n",
    "    def _load_latest_checkpoint(self):\n",
    "        \"\"\"Load latest checkpoint and prepare for resume.\"\"\"\n",
    "        print(f\"üìÇ Found checkpoint: {self.latest_checkpoint_path}\")\n",
    "        try:\n",
    "            self.loaded_checkpoint = torch.load(self.latest_checkpoint_path, map_location='cpu', weights_only=False)\n",
    "            self.start_epoch = self.loaded_checkpoint.get('epoch', 0) + 1\n",
    "            \n",
    "            # Load history if available\n",
    "            if 'history' in self.loaded_checkpoint:\n",
    "                history_dict = self.loaded_checkpoint['history']\n",
    "                # Reconstruct TrainingHistory from dict\n",
    "                self.history = TrainingHistory()\n",
    "                for key, value in history_dict.items():\n",
    "                    if hasattr(self.history, key):\n",
    "                        setattr(self.history, key, value)\n",
    "            \n",
    "            if self.verbose: \n",
    "                print(f\"‚úÖ Will resume from epoch {self.start_epoch}\")\n",
    "                print(f\"üìà Loaded {len(self.history)} epochs of training history\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load checkpoint: {e}\")\n",
    "            print(\"üÜï Starting fresh training\")\n",
    "            self.loaded_checkpoint = None\n",
    "            self.start_epoch = 1\n",
    "    \n",
    "    def _save_latest_checkpoint(self, model, optimizer, scaler, scheduler, epoch, metrics):\n",
    "        \"\"\"Save latest checkpoint (every epoch).\"\"\"\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"history\": self.history.__dict__,\n",
    "            \"config\": self.config,\n",
    "            \"metrics\": metrics,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, self.latest_checkpoint_path)\n",
    "    \n",
    "    def _save_best_model(self, model, optimizer, scheduler, epoch, metrics, best_miou):\n",
    "        \"\"\"Save best model checkpoint.\"\"\"\n",
    "        best_checkpoint_path = self.config.model_dir / \"best.pth\"\n",
    "        best_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"config\": self.config,\n",
    "            \"metrics\": metrics,\n",
    "            \"history\": self.history,\n",
    "            \"best_miou\": best_miou\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "        print(f\"‚úÖ Saved best model to {best_checkpoint_path} (mIoU: {best_miou:.4f})\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model, loader=None, criterion=None):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation data.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to evaluate\n",
    "            loader: DataLoader to use (defaults to self.val_loader)\n",
    "            criterion: Loss function to use for validation loss calculation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics including loss if criterion provided\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        loader = loader or self.val_loader\n",
    "        \n",
    "        cm = np.zeros((self.config.num_classes, self.config.num_classes), dtype=np.int64)\n",
    "        total, correct = 0, 0\n",
    "        running_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            # Handle different batch formats\n",
    "            if \"pixel_values\" in batch:\n",
    "                # SegFormer format\n",
    "                inputs = batch[\"pixel_values\"].to(self.device, non_blocking=True)\n",
    "                targets = batch[\"labels\"].to(self.device, non_blocking=True)\n",
    "            elif \"image\" in batch:\n",
    "                # TorchVision format\n",
    "                inputs = batch[\"image\"].to(self.device, non_blocking=True)\n",
    "                targets = batch[\"mask\"].to(self.device, non_blocking=True)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown batch format. Expected 'pixel_values' or 'image' keys.\")\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type=self.device.type, enabled=(self.device.type == \"cuda\")):\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Handle different model output formats\n",
    "                if hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits  # SegFormer format\n",
    "                elif isinstance(outputs, dict) and \"out\" in outputs:\n",
    "                    logits = outputs[\"out\"]  # TorchVision DeepLab format\n",
    "                else:\n",
    "                    logits = outputs  # Standard tensor output\n",
    "                \n",
    "                # Upsample logits to target size if needed\n",
    "                if logits.shape[-2:] != targets.shape[-2:]:\n",
    "                    logits = torch.nn.functional.interpolate(\n",
    "                        logits, size=targets.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "                    )\n",
    "                \n",
    "                # Calculate validation loss if criterion provided\n",
    "                if criterion is not None:\n",
    "                    val_loss = criterion(logits, targets)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    num_batches += 1\n",
    "            \n",
    "            pred = logits.argmax(1)  # [B,H,W]\n",
    "            \n",
    "            valid = targets != self.config.ignore_index\n",
    "            total += valid.sum().item()\n",
    "            correct += (pred[valid] == targets[valid]).sum().item()\n",
    "            \n",
    "            p = pred[valid].view(-1).cpu().numpy()\n",
    "            t = targets[valid].view(-1).cpu().numpy()\n",
    "            for i in range(p.shape[0]):\n",
    "                if 0 <= t[i] < self.config.num_classes and 0 <= p[i] < self.config.num_classes:\n",
    "                    cm[t[i], p[i]] += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inter = np.diag(cm).astype(np.float64)\n",
    "        gt = cm.sum(1).astype(np.float64)\n",
    "        pr = cm.sum(0).astype(np.float64)\n",
    "        union = gt + pr - inter\n",
    "        iou = inter / np.maximum(union, 1)\n",
    "        miou = float(np.nanmean(iou)) if iou.size > 0 else 0.0\n",
    "        pixacc = float(correct / max(total, 1))\n",
    "        \n",
    "        metrics = {\"mIoU\": miou, \"PixelAcc\": pixacc, \"IoU_per_class\": iou}\n",
    "        \n",
    "        # Add validation loss if calculated\n",
    "        if criterion is not None and num_batches > 0:\n",
    "            avg_val_loss = running_val_loss / num_batches\n",
    "            metrics[\"loss\"] = avg_val_loss\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(self, model, optimizer=None, scheduler=None, criterion=None,\n",
    "              gradient_clip_val=1.0, early_stopping_patience=None):\n",
    "        \"\"\"\n",
    "        Train the provided model with automatic checkpoint resume.\n",
    "\n",
    "        Args:\n",
    "            model: PyTorch model to train\n",
    "            optimizer: Custom optimizer (defaults to AdamW)\n",
    "            scheduler: Custom scheduler (defaults to None)\n",
    "            criterion: Custom loss function (defaults to CrossEntropyLoss)\n",
    "            gradient_clip_val: Gradient clipping value (None to disable)\n",
    "            early_stopping_patience: Stop training if no improvement for N epochs (None to disable)\n",
    "\n",
    "        Returns:\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        # Prepare model\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        # Setup training components with defaults if not provided\n",
    "        if criterion is None:\n",
    "            criterion = nn.CrossEntropyLoss(ignore_index=self.config.ignore_index)\n",
    "            \n",
    "        if optimizer is None:\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), lr=self.config.learning_rate\n",
    "            )\n",
    "            print(f\"üìä Using default AdamW optimizer (LR: {self.config.learning_rate:.2e})\")\n",
    "        else:\n",
    "            print(f\"üìä Using custom optimizer: {type(optimizer).__name__}\")\n",
    "            \n",
    "        scaler = GradScaler(enabled=(self.device.type == \"cuda\"))\n",
    "        \n",
    "        if scheduler:\n",
    "            print(f\"üìà Using custom scheduler: {type(scheduler).__name__}\")\n",
    "        else:\n",
    "            print(\"üìà No LR scheduling (constant learning rate)\")\n",
    "        \n",
    "        # Early stopping setup\n",
    "        if early_stopping_patience is not None:\n",
    "            print(f\"üõë Early stopping enabled (patience: {early_stopping_patience} epochs)\")\n",
    "            best_miou_for_early_stop = -1.0\n",
    "            patience_counter = 0\n",
    "            min_delta = 0.001  # Minimum improvement threshold\n",
    "        \n",
    "        # Load checkpoint states if resuming\n",
    "        if self.loaded_checkpoint:\n",
    "            print(\"üîÑ Restoring training state from checkpoint...\")\n",
    "            \n",
    "            # Load model state\n",
    "            model.load_state_dict(self.loaded_checkpoint['model_state_dict'])\n",
    "            \n",
    "            # Load optimizer state\n",
    "            if 'optimizer_state_dict' in self.loaded_checkpoint:\n",
    "                optimizer.load_state_dict(self.loaded_checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "            # Load scheduler state\n",
    "            if scheduler and 'scheduler_state_dict' in self.loaded_checkpoint and self.loaded_checkpoint['scheduler_state_dict']:\n",
    "                scheduler.load_state_dict(self.loaded_checkpoint['scheduler_state_dict'])\n",
    "            \n",
    "            # Load scaler state\n",
    "            if 'scaler_state_dict' in self.loaded_checkpoint:\n",
    "                scaler.load_state_dict(self.loaded_checkpoint['scaler_state_dict'])\n",
    "            \n",
    "            print(\"‚úÖ Training state restored\")\n",
    "        \n",
    "        # Print parameter information\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        if gradient_clip_val is not None:\n",
    "            print(f\"‚úÇÔ∏è Gradient clipping enabled (max_norm: {gradient_clip_val})\")\n",
    "        \n",
    "        # Training setup\n",
    "        best_miou = max(self.history.val_miou) if self.history.val_miou else -1.0\n",
    "        \n",
    "        print(f\"üöÄ Starting training for {self.config.epochs} epochs...\")\n",
    "        print(f\"üìä Training batches: {len(self.train_loader)}, Validation batches: {len(self.val_loader)}\")\n",
    "        print(f\"‚ñ∂Ô∏è Resuming from epoch {self.start_epoch}\")\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.config.epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            print(f\"\\nüìà Epoch {epoch}/{self.config.epochs}\")\n",
    "            \n",
    "            for it, batch in enumerate(self.train_loader, start=1):\n",
    "                # Handle different batch formats\n",
    "                if \"pixel_values\" in batch:\n",
    "                    # SegFormer format\n",
    "                    inputs = batch[\"pixel_values\"].to(self.device, non_blocking=True)\n",
    "                    targets = batch[\"labels\"].to(self.device, non_blocking=True)\n",
    "                elif \"image\" in batch:\n",
    "                    # TorchVision format\n",
    "                    inputs = batch[\"image\"].to(self.device, non_blocking=True)\n",
    "                    targets = batch[\"mask\"].to(self.device, non_blocking=True)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown batch format. Expected 'pixel_values' or 'image' keys.\")\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast(device_type=self.device.type, enabled=(self.device.type == \"cuda\")):\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Handle different model output formats\n",
    "                    if hasattr(outputs, 'logits'):\n",
    "                        logits = outputs.logits  # SegFormer format\n",
    "                    elif isinstance(outputs, dict) and \"out\" in outputs:\n",
    "                        logits = outputs[\"out\"]  # TorchVision DeepLab format\n",
    "                    else:\n",
    "                        logits = outputs  # Standard tensor output\n",
    "                    \n",
    "                    # Upsample logits to target size if needed\n",
    "                    if logits.shape[-2:] != targets.shape[-2:]:\n",
    "                        logits = torch.nn.functional.interpolate(\n",
    "                            logits, size=targets.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "                        )\n",
    "                    \n",
    "                    loss = criterion(logits, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if gradient_clip_val is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Log periodically\n",
    "                if it % self.config.log_every == 0:\n",
    "                    avg_loss = running_loss / it\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"   Batch {it:05d}/{len(self.train_loader)} | Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "            # Step scheduler if provided\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Validation\n",
    "            print(\"üîç Running validation...\")\n",
    "            val_metrics = self.evaluate(model, criterion=criterion)\n",
    "            current_miou = val_metrics[\"mIoU\"]\n",
    "            \n",
    "            # Early stopping check\n",
    "            if early_stopping_patience is not None:\n",
    "                if current_miou > best_miou_for_early_stop + min_delta:\n",
    "                    best_miou_for_early_stop = current_miou\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                print(f\"üìä Early stopping: {patience_counter}/{early_stopping_patience} (best: {best_miou_for_early_stop:.4f})\")\n",
    "                \n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"üõë Early stopping triggered at epoch {epoch}\")\n",
    "                    print(f\"   No improvement in validation mIoU for {early_stopping_patience} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            avg_train_loss = running_loss / len(self.train_loader)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Add to history\n",
    "            self.history.add_epoch({\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_miou\": 0.0,  # Not calculated during training for performance\n",
    "                \"train_pixacc\": 0.0,  # Not calculated during training for performance\n",
    "                \"val_loss\": val_metrics.get(\"loss\", 0.0),\n",
    "                \"val_miou\": val_metrics[\"mIoU\"],\n",
    "                \"val_pixacc\": val_metrics[\"PixelAcc\"],\n",
    "                \"val_iou_per_class\": val_metrics[\"IoU_per_class\"].tolist(),\n",
    "                \"learning_rate\": current_lr,\n",
    "                \"epoch_time\": epoch_time\n",
    "            })\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nüìä Epoch {epoch} Results:\")\n",
    "            print(f\"   ‚Ä¢ Train Loss: {avg_train_loss:.4f}\")\n",
    "            if \"loss\" in val_metrics:\n",
    "                print(f\"   ‚Ä¢ Val Loss: {val_metrics['loss']:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Val mIoU: {val_metrics['mIoU']:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Val PixelAcc: {val_metrics['PixelAcc']:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Epoch Time: {epoch_time:.1f}s\")\n",
    "            print(f\"   ‚Ä¢ Learning Rate: {current_lr:.2e}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics[\"mIoU\"] > best_miou:\n",
    "                best_miou = val_metrics[\"mIoU\"]\n",
    "                self._save_best_model(model, optimizer, scheduler, epoch, val_metrics, best_miou)\n",
    "            \n",
    "            # Save latest checkpoint every epoch\n",
    "            self._save_latest_checkpoint(model, optimizer, scaler, scheduler, epoch, val_metrics)\n",
    "            \n",
    "            # Save every 5 epochs \n",
    "            if epoch % 5 == 0:\n",
    "                epoch_checkpoint_path = self.config.model_dir / f\"checkpoint_epoch_{epoch}.pth\"\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "                    \"scaler_state_dict\": scaler.state_dict(),\n",
    "                    \"history\": self.history.__dict__,\n",
    "                    \"config\": self.config,\n",
    "                    \"metrics\": val_metrics,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }, epoch_checkpoint_path)\n",
    "                print(f\"üíæ Saved periodic checkpoint to {epoch_checkpoint_path}\")\n",
    "\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"\\nüéâ Training Complete!\")\n",
    "        print(f\"   ‚Ä¢ Best Validation mIoU: {best_miou:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Total Training Time: {sum(self.history.epoch_times):.1f}s\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "print(\"‚úÖ Universal Trainer class with dual model support defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_status(model):\n",
    "    \"\"\"Show which parts of the model are frozen/unfrozen.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    frozen_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"üìä Model status: {trainable_params:,} trainable / {total_params:,} total\")\n",
    "    print(f\"   ({frozen_params:,} frozen parameters)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([40]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([40, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model status: 405,032 trainable / 3,724,424 total\n",
      "   (3,319,392 frozen parameters)\n"
     ]
    }
   ],
   "source": [
    "segformer_training_config = TrainingConfig(\n",
    "    data_root=DATA_ROOT,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,  # Conservative learning rate\n",
    "    model_name=\"segformer_b0\",\n",
    "    processor=SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    ")\n",
    "\n",
    "# Create fresh SegFormer model\n",
    "segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "    num_labels=40,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# freeze all encoder layers\n",
    "for param in segformer_model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print_model_status(segformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: TrainingHistory):\n",
    "    \"\"\"Plot training and validation loss and mIoU over epochs.\"\"\"\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    ax1.plot(epochs, history.train_loss, label='Train Loss', color='blue')\n",
    "    ax1.plot(epochs, history.val_loss, label='Val Loss', color='orange')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot mIoU\n",
    "    ax2.plot(epochs, history.val_miou, label='Val mIoU', color='green')\n",
    "    ax2.plot(epochs,history.val_pixacc, label='Val PixelAcc', color='red')\n",
    "    ax2.set_title('Validation mIoU')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('mIoU')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create cosine annealing scheduler\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optim.AdamW(segformer_model.parameters(), lr=segformer_training_config.learning_rate),\n",
    "#     T_max=segformer_training_config.epochs\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(segformer_training_config)\n",
    "# trained_model = trainer.train(segformer_model, scheduler=scheduler, early_stopping_patience=5)\n",
    "\n",
    "# plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers in the classifier:\n",
      "Layer 0: ASPP\n",
      "Layer 1: Conv2d\n",
      "Layer 2: BatchNorm2d\n",
      "Layer 3: ReLU\n",
      "Layer 4: Conv2d\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights # fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "dl_mnv3large_training_config = TrainingConfig(\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    model_name=\"deeplabv3_mobilenet_v3_large\",\n",
    "    processor=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT.transforms()\n",
    ")\n",
    "\n",
    "model_dl_mnv3large = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "\n",
    "# Modify the final layer to match NYU40 classes\n",
    "model_dl_mnv3large.classifier[-1] = nn.Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1))  # 40 classes for NYU40\n",
    "\n",
    "print(\"Layers in the classifier:\")\n",
    "for i, layer in enumerate(model_dl_mnv3large.classifier):\n",
    "    print(f\"Layer {i}: {type(layer).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Using custom processor: SemanticSegmentation\n",
      "SemanticSegmentation(\n",
      "    resize_size=[520]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "SemanticSegmentation(\n",
      "    resize_size=[520]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "SemanticSegmentation(\n",
      "    resize_size=[520]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 80,  Val batches: 20,  Test batches: 82\n",
      "\\nüß™ Testing data loading...\n",
      "‚úÖ Batch loaded successfully!\n",
      "\\nüñºÔ∏è Visualizing batch samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reubendrummond/Desktop/Uni/2025/Sem2/EGH444/EGH444-Group-10/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders for exploration\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    data_root=DATA_ROOT,\n",
    "    processor=dl_mnv3large_training_config.processor,\n",
    "    batch_size=8,\n",
    "    image_size=(240, 320),\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Test data loading and visualization\n",
    "print(\"\\\\nüß™ Testing data loading...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"‚úÖ Batch loaded successfully!\")\n",
    "\n",
    "# Visualize samples for Segformer\n",
    "print(\"\\\\nüñºÔ∏è Visualizing batch samples...\")\n",
    "# visualize_batch(sample_batch, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model_dl_mnv3large.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Phase 1: train only classifier head\n",
    "print(\"Unfreezing classifier: final classifier layer\")\n",
    "for param in model_dl_mnv3large.classifier[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "print_model_status(model_dl_mnv3large)\n",
    "dl_mnv3large_training_config.epochs = 20\n",
    "trainer = Trainer(dl_mnv3large_training_config)\n",
    "trainer.train(model_dl_mnv3large, early_stopping_patience=5)\n",
    "plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Unfreeze ASPP and classifier head\n",
    "print(\"Unfreezing classifier: ASPP and classifier head\")\n",
    "for param in model_dl_mnv3large.classifier[0].parameters():\n",
    "    param.requires_grad = True\n",
    "print_model_status(model_dl_mnv3large)\n",
    "\n",
    "dl_mnv3large_training_config.epochs = 30\n",
    "dl_mnv3large_training_config.learning_rate = 5e-5  # Lower learning rate for fine-tuning\n",
    "trainer = Trainer(dl_mnv3large_training_config)\n",
    "trainer.train(model_dl_mnv3large, early_stopping_patience=5)\n",
    "plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Unfreeze Conv block\n",
    "print(\"Unfreezing classifier: Conv block, ASPP, and classifier head\")\n",
    "for param in model_dl_mnv3large.classifier[1].parameters():\n",
    "    param.requires_grad = True\n",
    "print_model_status(model_dl_mnv3large)\n",
    "\n",
    "dl_mnv3large_training_config.epochs = 30\n",
    "dl_mnv3large_training_config.learning_rate = 1e-5  # Lower learning rate for fine-tuning\n",
    "trainer = Trainer(dl_mnv3large_training_config)\n",
    "trainer.train(model_dl_mnv3large, early_stopping_patience=5)\n",
    "plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: All encoder unfrozen (ie unfreeze BN layer only)\n",
    "print(\"Unfreezing classifier: Conv block, ASPP, and classifier head all classifier layers\")\n",
    "for param in model_dl_mnv3large.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "print_model_status(model_dl_mnv3large)\n",
    "\n",
    "dl_mnv3large_training_config.epochs = 20\n",
    "trainer = Trainer(dl_mnv3large_training_config)\n",
    "trainer.train(model_dl_mnv3large, early_stopping_patience=5)\n",
    "plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: unfreeze all params\n",
    "print(\"Unfreezing entire model for fine-tuning\")\n",
    "for param in model_dl_mnv3large.parameters():\n",
    "    param.requires_grad = True\n",
    "print_model_status(model_dl_mnv3large)\n",
    "dl_mnv3large_training_config.epochs = 200\n",
    "dl_mnv3large_training_config.learning_rate = 5e-6  # Lower learning rate for unfrozen training\n",
    "trainer = Trainer(dl_mnv3large_training_config)\n",
    "trainer.train(model_dl_mnv3large, early_stopping_patience=5)\n",
    "plot_training_history(trainer.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelEvaluator class defined!\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Lightweight evaluator for loading and testing trained models from \n",
    "checkpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, model, checkpoint_dir_name):\n",
    "        \"\"\"\n",
    "        Initialize evaluator with a model and checkpoint directory.\n",
    "        \n",
    "        Args:\n",
    "            model: The model instance to evaluate (should match checkpoint \n",
    "architecture)\n",
    "            checkpoint_dir_name: Name of the checkpoint directory (e.g., \n",
    "\"deeplabv3_resnet50\")\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.checkpoint_dir = Path(\"checkpoints\") / checkpoint_dir_name\n",
    "        self.device = self._setup_device()\n",
    "        self.config = None\n",
    "        self.loaded_checkpoint = None\n",
    "\n",
    "    def _setup_device(self) -> torch.device:\n",
    "        \"\"\"Setup evaluation device (reuse Trainer logic).\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"cpu\")  # Temporary fix for MPS issues\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        print(f\"üîß Evaluation device: {device}\")\n",
    "        return device\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_name=\"best.pth\"):\n",
    "        \"\"\"\n",
    "        Load model weights and config from checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_name: Name of checkpoint file (\"best.pth\", \"latest.pth\", etc.)\n",
    "        \"\"\"\n",
    "        checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
    "\n",
    "        if not checkpoint_path.exists():\n",
    "            raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "        print(f\"üìÇ Loading checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        # Load checkpoint\n",
    "        self.loaded_checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "        # Extract config\n",
    "        self.config = self.loaded_checkpoint['config']\n",
    "\n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(self.loaded_checkpoint['model_state_dict'])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Print checkpoint info\n",
    "        epoch = self.loaded_checkpoint.get('epoch', 'Unknown')\n",
    "        metrics = self.loaded_checkpoint.get('metrics', {})\n",
    "        print(f\"‚úÖ Loaded checkpoint from epoch {epoch}\")\n",
    "        if 'mIoU' in metrics:\n",
    "            print(f\"üìä Checkpoint mIoU: {metrics['mIoU']:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate_on_sets(self):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation and test sets using saved config.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with validation and test results\n",
    "        \"\"\"\n",
    "        if self.config is None:\n",
    "            raise ValueError(\"Must load checkpoint first using load_checkpoint()\")\n",
    "\n",
    "        print(f\"üîç Creating data loaders from saved config...\")\n",
    "\n",
    "        # Create data loaders using saved config\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            data_root=self.config.data_root,\n",
    "            processor=self.config.processor,\n",
    "            batch_size=self.config.batch_size,\n",
    "            image_size=self.config.image_size,\n",
    "            num_workers=self.config.num_workers,\n",
    "            pin_memory=self.config.pin_memory\n",
    "        )\n",
    "\n",
    "        # Create a minimal trainer instance just for the evaluate method\n",
    "\n",
    "        temp_trainer = Trainer(self.config, verbose=False)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        print(f\"\\nüìä Evaluating on validation set...\")\n",
    "        val_metrics = temp_trainer.evaluate(self.model, val_loader)\n",
    "\n",
    "        # Evaluate on test set  \n",
    "        print(f\"üìä Evaluating on test set...\")\n",
    "        test_metrics = temp_trainer.evaluate( self.model, test_loader)\n",
    "\n",
    "        # Format results\n",
    "        results = {\n",
    "            \"validation\": val_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"checkpoint_info\": {\n",
    "                \"epoch\": self.loaded_checkpoint.get('epoch'),\n",
    "                \"checkpoint_metrics\": self.loaded_checkpoint.get('metrics', {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\nüéØ Evaluation Results:\")\n",
    "        print(f\"{'Set':<12} {'mIoU':<8} {'PixelAcc':<10}\")\n",
    "        print(\"-\" * 32)\n",
    "        print(f\"{'Validation':<12} {val_metrics['mIoU']:<8.4f} {val_metrics['PixelAcc']:<10.4f}\")\n",
    "        print(f\"{'Test':<12} {test_metrics['mIoU']:<8.4f} {test_metrics['PixelAcc']:<10.4f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ ModelEvaluator class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Evaluation device: cpu\n",
      "üìÇ Loading checkpoint: checkpoints/segformer_b0/best.pth\n",
      "‚úÖ Loaded checkpoint from epoch 84\n",
      "üìä Checkpoint mIoU: 0.2439\n",
      "üîç Creating data loaders from saved config...\n",
      "üìè Using SegFormer processor with size: {'height': 512, 'width': 512}\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 20,  Val batches: 5,  Test batches: 21\n",
      "Using device: cpu\n",
      "üé≤ Random seeds set to 42 for reproducibility\n",
      "üìè Using SegFormer processor with size: {'height': 512, 'width': 512}\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 20,  Val batches: 5,  Test batches: 21\n",
      "üìÇ Found checkpoint: checkpoints/segformer_b0/latest.pth\n",
      "‚ùå Failed to load checkpoint: 'Trainer' object has no attribute 'verbose'\n",
      "üÜï Starting fresh training\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "üìä Evaluating on test set...\n",
      "\n",
      "üéØ Evaluation Results:\n",
      "Set          mIoU     PixelAcc  \n",
      "--------------------------------\n",
      "Validation   0.2439   0.6611    \n",
      "Test         0.3317   0.6499    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validation': {'mIoU': 0.24392526795392336,\n",
       "  'PixelAcc': 0.6611169348869812,\n",
       "  'IoU_per_class': array([0.37849045, 0.69339775, 0.71812873, 0.36620149, 0.72410672,\n",
       "         0.46212947, 0.56167901, 0.42161724, 0.17991459, 0.34473374,\n",
       "         0.25828629, 0.53749807, 0.14937577, 0.37125376, 0.05110136,\n",
       "         0.06615942, 0.30263649, 0.20773736, 0.26192   , 0.10157913,\n",
       "         0.30809409, 0.2482833 , 0.61231308, 0.12936746, 0.        ,\n",
       "         0.30012957, 0.20733366, 0.        , 0.        , 0.02480961,\n",
       "         0.        , 0.26239706, 0.13460407, 0.        , 0.        ,\n",
       "         0.15390921, 0.        , 0.        , 0.14669873, 0.07112406])},\n",
       " 'test': {'mIoU': 0.331694428039324,\n",
       "  'PixelAcc': 0.6499031482781346,\n",
       "  'IoU_per_class': array([0.39428265, 0.66724136, 0.71529508, 0.52249209, 0.65283061,\n",
       "         0.45790927, 0.55360578, 0.34235176, 0.257325  , 0.38311996,\n",
       "         0.36877846, 0.53099914, 0.47119343, 0.50339252, 0.14686693,\n",
       "         0.06762825, 0.49665052, 0.42461858, 0.31185065, 0.28047105,\n",
       "         0.12427903, 0.14962996, 0.50852077, 0.18677175, 0.38536365,\n",
       "         0.42397451, 0.19950889, 0.31566571, 0.23179266, 0.04465686,\n",
       "         0.00177357, 0.42683337, 0.16680307, 0.53482161, 0.34986763,\n",
       "         0.23449087, 0.21446197, 0.        , 0.17990603, 0.03975208])},\n",
       " 'checkpoint_info': {'epoch': 84,\n",
       "  'checkpoint_metrics': {'mIoU': 0.24392526795392336,\n",
       "   'PixelAcc': 0.6611169348869812,\n",
       "   'IoU_per_class': array([0.37849045, 0.69339775, 0.71812873, 0.36620149, 0.72410672,\n",
       "          0.46212947, 0.56167901, 0.42161724, 0.17991459, 0.34473374,\n",
       "          0.25828629, 0.53749807, 0.14937577, 0.37125376, 0.05110136,\n",
       "          0.06615942, 0.30263649, 0.20773736, 0.26192   , 0.10157913,\n",
       "          0.30809409, 0.2482833 , 0.61231308, 0.12936746, 0.        ,\n",
       "          0.30012957, 0.20733366, 0.        , 0.        , 0.02480961,\n",
       "          0.        , 0.26239706, 0.13460407, 0.        , 0.        ,\n",
       "          0.15390921, 0.        , 0.        , 0.14669873, 0.07112406]),\n",
       "   'loss': 1.082731795310974}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_model_eval = ModelEvaluator(segformer_model, \"segformer_b0\")\n",
    "\n",
    "segformer_model_eval.load_checkpoint(\"best.pth\")\n",
    "segformer_model_eval.evaluate_on_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Evaluation device: cpu\n",
      "üìÇ Loading checkpoint: checkpoints/deeplabv3_resnet50/best.pth\n",
      "‚úÖ Loaded checkpoint from epoch 1\n",
      "üìä Checkpoint mIoU: 0.0037\n",
      "üîç Creating data loaders from saved config...\n",
      "üìè Using custom processor: SemanticSegmentation\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 20,  Val batches: 5,  Test batches: 21\n",
      "Using device: cpu\n",
      "üé≤ Random seeds set to 42 for reproducibility\n",
      "üìè Using custom processor: SemanticSegmentation\n",
      "üìä Train: 636 samples,  Val: 159 samples,  Test: 654 samples\n",
      "üì¶ Train batches: 20,  Val batches: 5,  Test batches: 21\n",
      "üìÇ Found checkpoint: checkpoints/deeplabv3_resnet50/latest.pth\n",
      "‚ùå Failed to load checkpoint: 'Trainer' object has no attribute 'verbose'\n",
      "üÜï Starting fresh training\n",
      "\n",
      "üìä Evaluating on validation set...\n",
      "üìä Evaluating on test set...\n",
      "\n",
      "üéØ Evaluation Results:\n",
      "Set          mIoU     PixelAcc  \n",
      "--------------------------------\n",
      "Validation   0.0037   0.0253    \n",
      "Test         0.0046   0.0250    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'validation': {'mIoU': 0.0036749019225885164,\n",
       "  'PixelAcc': 0.025326054077795257,\n",
       "  'IoU_per_class': array([2.01587630e-02, 3.08297507e-02, 8.08725702e-03, 2.28449855e-02,\n",
       "         1.05620294e-03, 1.49315430e-03, 0.00000000e+00, 1.50002625e-05,\n",
       "         2.34294558e-04, 0.00000000e+00, 2.53473691e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.06587878e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.55034307e-02, 0.00000000e+00, 1.62816729e-05, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.24630109e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.49115250e-04, 0.00000000e+00, 0.00000000e+00, 4.45306490e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.27164625e-03, 2.44330405e-03])},\n",
       " 'test': {'mIoU': 0.00463618206942854,\n",
       "  'PixelAcc': 0.02498658294723772,\n",
       "  'IoU_per_class': array([3.37432214e-02, 2.98771322e-02, 8.09790839e-03, 2.67364434e-02,\n",
       "         2.24265170e-03, 1.20663976e-04, 0.00000000e+00, 1.37734988e-02,\n",
       "         1.06702549e-02, 5.28986242e-06, 1.45293680e-02, 0.00000000e+00,\n",
       "         4.41886955e-05, 9.09903877e-03, 2.34398074e-05, 4.38961062e-03,\n",
       "         4.13724822e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.06196124e-03, 0.00000000e+00, 1.49521531e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.02503805e-04, 0.00000000e+00, 2.26237284e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.10861698e-03, 0.00000000e+00, 0.00000000e+00, 3.38127319e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.13977061e-02, 7.14355029e-03])},\n",
       " 'checkpoint_info': {'epoch': 1,\n",
       "  'checkpoint_metrics': {'mIoU': 0.0036749019225885164,\n",
       "   'PixelAcc': 0.025326054077795257,\n",
       "   'IoU_per_class': array([2.01587630e-02, 3.08297507e-02, 8.08725702e-03, 2.28449855e-02,\n",
       "          1.05620294e-03, 1.49315430e-03, 0.00000000e+00, 1.50002625e-05,\n",
       "          2.34294558e-04, 0.00000000e+00, 2.53473691e-03, 0.00000000e+00,\n",
       "          0.00000000e+00, 3.06587878e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.55034307e-02, 0.00000000e+00, 1.62816729e-05, 0.00000000e+00,\n",
       "          0.00000000e+00, 1.24630109e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.49115250e-04, 0.00000000e+00, 0.00000000e+00, 4.45306490e-03,\n",
       "          0.00000000e+00, 0.00000000e+00, 5.27164625e-03, 2.44330405e-03]),\n",
       "   'loss': 3.602675533294678}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dl_mnv3large_eval = ModelEvaluator(model_dl_mnv3large, \"deeplabv3_resnet50\")\n",
    "model_dl_mnv3large_eval.load_checkpoint(\"best.pth\")\n",
    "model_dl_mnv3large_eval.evaluate_on_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, dataset, device, sample_indices=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Get model predictions for specified samples.\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with keys: 'rgb', 'ground_truth', 'prediction', 'sample_id'\n",
    "    \"\"\"\n",
    "    \n",
    "    def denormalize_imagenet(tensor: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Denormalize ImageNet preprocessed image.\"\"\"\n",
    "        IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "        IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        img = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img * IMAGENET_STD + IMAGENET_MEAN).clip(0, 1)\n",
    "        return (img * 255).astype(np.uint8)\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in sample_indices:\n",
    "            # Get original image size by loading raw image\n",
    "            stem, rgb_path, label_path = dataset.items[idx]\n",
    "            \n",
    "            # Load original image to get true dimensions\n",
    "            import cv2\n",
    "            original_rgb = cv2.imread(str(rgb_path), cv2.IMREAD_COLOR)\n",
    "            original_rgb = cv2.cvtColor(original_rgb, cv2.COLOR_BGR2RGB)\n",
    "            original_height, original_width = original_rgb.shape[:2]\n",
    "            \n",
    "            # Get processed sample from dataset\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Extract data based on format\n",
    "            if \"pixel_values\" in sample:\n",
    "                # SegFormer format\n",
    "                image_tensor = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "                label = sample[\"labels\"].cpu().numpy()\n",
    "                sample_id = sample[\"id\"]\n",
    "            elif \"image\" in sample:\n",
    "                # TorchVision format\n",
    "                image_tensor = sample[\"image\"].unsqueeze(0).to(device)\n",
    "                label = sample[\"mask\"].cpu().numpy()\n",
    "                sample_id = sample[\"id\"]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown sample format\")\n",
    "            \n",
    "            # Get model prediction\n",
    "            outputs = model(image_tensor)\n",
    "            \n",
    "            # Handle different model output formats\n",
    "            if hasattr(outputs, 'logits'):\n",
    "                logits = outputs.logits  # SegFormer\n",
    "            elif isinstance(outputs, dict) and \"out\" in outputs:\n",
    "                logits = outputs[\"out\"]  # TorchVision DeepLab\n",
    "            else:\n",
    "                logits = outputs  # Standard tensor\n",
    "            \n",
    "            # Always resize prediction to ORIGINAL image size for fair comparison\n",
    "            if logits.shape[-2:] != (original_height, original_width):\n",
    "                logits = torch.nn.functional.interpolate(\n",
    "                    logits, size=(original_height, original_width), mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "            \n",
    "            prediction = logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Denormalize input image and resize to original size for display\n",
    "            rgb_display = denormalize_imagenet(image_tensor.squeeze(0))\n",
    "            if rgb_display.shape[:2] != (original_height, original_width):\n",
    "                rgb_display = cv2.resize(rgb_display, (original_width, original_height), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # Also resize ground truth label to original size for consistent comparison\n",
    "            if label.shape != (original_height, original_width):\n",
    "                label_resized = cv2.resize(\n",
    "                    label.astype(np.uint8), (original_width, original_height), interpolation=cv2.INTER_NEAREST\n",
    "                ).astype(np.int64)\n",
    "            else:\n",
    "                label_resized = label\n",
    "            \n",
    "            results.append({\n",
    "                'rgb': rgb_display,\n",
    "                'ground_truth': label_resized,\n",
    "                'prediction': prediction,\n",
    "                'sample_id': sample_id,\n",
    "                'original_size': (original_height, original_width)\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-model prediction and visualization functions defined!\n"
     ]
    }
   ],
   "source": [
    "def visualize_multi_model_predictions(predictions_dict, num_classes=40):\n",
    "    \"\"\"\n",
    "    Visualize predictions from multiple models side by side.\n",
    "    \n",
    "    Args:\n",
    "        predictions_dict: Dict where keys are model names and values are prediction results\n",
    "        num_classes: Number of classes for colorization\n",
    "    \"\"\"\n",
    "    \n",
    "    def colorize_mask(mask: np.ndarray, num_classes: int = 40) -> np.ndarray:\n",
    "        \"\"\"Create colored visualization of segmentation mask.\"\"\"\n",
    "        np.random.seed(42)  # Consistent colors across models\n",
    "        colors = np.random.randint(0, 255, (num_classes + 1, 3), dtype=np.uint8)\n",
    "        colors[-1] = [128, 128, 128]  # Ignore class = gray\n",
    "        \n",
    "        colored = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for class_id in range(num_classes + 1):\n",
    "            if class_id == 255:  # Handle ignore index\n",
    "                colored[mask == 255] = [128, 128, 128]\n",
    "            else:\n",
    "                colored[mask == class_id] = colors[class_id]\n",
    "        return colored\n",
    "    \n",
    "    model_names = list(predictions_dict.keys())\n",
    "    num_models = len(model_names)\n",
    "    \n",
    "    # Get number of samples from first model's predictions\n",
    "    num_samples = len(predictions_dict[model_names[0]])\n",
    "    \n",
    "    # Rows: RGB + Ground Truth + Model predictions\n",
    "    num_rows = 2 + num_models  \n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_samples, figsize=(num_samples * 4, num_rows * 3))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Use first model's data for RGB and ground truth (should be same across models)\n",
    "        first_model_data = predictions_dict[model_names[0]][i]\n",
    "        \n",
    "        # Plot RGB image\n",
    "        axes[0, i].imshow(first_model_data['rgb'])\n",
    "        axes[0, i].set_title(f\"RGB\\n{first_model_data['sample_id']}\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        \n",
    "        # Plot ground truth\n",
    "        colored_gt = colorize_mask(first_model_data['ground_truth'], num_classes)\n",
    "        axes[1, i].imshow(colored_gt)\n",
    "        axes[1, i].set_title(\"Ground Truth\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "        \n",
    "        # Plot each model's prediction\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            prediction = predictions_dict[model_name][i]['prediction']\n",
    "            colored_pred = colorize_mask(prediction, num_classes)\n",
    "            \n",
    "            row_idx = 2 + j\n",
    "            axes[row_idx, i].imshow(colored_pred)\n",
    "            axes[row_idx, i].set_title(f\"{model_name}\")\n",
    "            axes[row_idx, i].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"Multi-Model Predictions Comparison\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Multi-model prediction and visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer_model.load_state_dict(torch.load(\"checkpoints/segformer_b0/best.pth\", weights_only=False, map_location=\"cpu\")['model_state_dict'])\n",
    "model_dl_mnv3large.load_state_dict(torch.load(\"checkpoints/deeplabv3_resnet50/dl_best2.pth\", weights_only=False, map_location=\"cpu\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/reubendrummond/.cache/huggingface/hub/models--nvidia--segformer-b0-finetuned-ade-512-512/snapshots/489d5cd81a0b59fab9b7ea758d3548ebe99677da/preprocessor_config.json\n",
      "/Users/reubendrummond/Desktop/Uni/2025/Sem2/EGH444/EGH444-Group-10/venv/lib/python3.10/site-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 512. Converted to {'height': 512, 'width': 512}.\n",
      "Image processor SegformerImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_reduce_labels\": false,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 512,\n",
      "    \"width\": 512\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegformerImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_reduce_labels\": false,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 512,\n",
      "    \"width\": 512\n",
      "  }\n",
      "}\n",
      "\n",
      "SemanticSegmentation(\n",
      "    resize_size=[520]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n",
      "üîÆ Getting SegFormer predictions...\n",
      "üîÆ Getting DeepLabV3 predictions...\n",
      "üñºÔ∏è Creating multi-model comparison visualization...\n",
      "‚úÖ Multi-model prediction comparison complete!\n"
     ]
    }
   ],
   "source": [
    "# Example Usage: Multi-Model Prediction Comparison\n",
    "\n",
    "# Step 1: Create datasets for each model\n",
    "segformer_processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "torchvision_processor = DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT.transforms()\n",
    "\n",
    "val_dataset_segformer = NYUDepthDataset(DATA_ROOT, \"test\", processor=segformer_processor)\n",
    "val_dataset_torchvision = NYUDepthDataset(DATA_ROOT, \"test\", processor=torchvision_processor)\n",
    "\n",
    "# Step 2: Get predictions from all models\n",
    "device = torch.device(\"cpu\")  # or \"cuda\" if available\n",
    "sample_indices = [0, 5, 10, 15, 20, 25]  # Which samples to compare\n",
    "\n",
    "print(\"üîÆ Getting SegFormer predictions...\")\n",
    "segformer_predictions = get_model_predictions(\n",
    "    segformer_model, val_dataset_segformer, device, sample_indices\n",
    ")\n",
    "\n",
    "print(\"üîÆ Getting DeepLabV3 predictions...\")\n",
    "deeplabv3_predictions = get_model_predictions(\n",
    "    model_dl_mnv3large, val_dataset_torchvision, device, sample_indices  \n",
    ")\n",
    "\n",
    "# Step 3: Visualize all models together\n",
    "predictions_dict = {\n",
    "    \"SegFormer-B0\": segformer_predictions,\n",
    "    \"DeepLabV3-ResNet50\": deeplabv3_predictions\n",
    "}\n",
    "\n",
    "print(\"üñºÔ∏è Creating multi-model comparison visualization...\")\n",
    "# visualize_multi_model_predictions(predictions_dict)  # Now automatically shows all samples\n",
    "\n",
    "print(\"‚úÖ Multi-model prediction comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
